{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.rdd import RDD\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('Distributed Learning of Finite Gaussian Mixtures') \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = pyspark.SQLContext(sc)\n",
    "#     .config(\"spark.driver.memory\", \"15g\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "import time\n",
    "from numpy import array\n",
    "# Répertoire courant ou répertoire accessible de tous les \"workers\" du cluster\n",
    "DATA_PATH=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des fichiers\n",
    "import urllib.request\n",
    "f = urllib.request.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_train.csv\",DATA_PATH+\"mnist_train.csv\")\n",
    "f = urllib.request.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_test.csv\",DATA_PATH+\"mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD = sc.textFile(DATA_PATH+\"mnist_train.csv\").map(lambda l: [float(x) for x in l.split(',')])\n",
    "testRDD = sc.textFile(DATA_PATH+'mnist_test.csv').map(lambda l: [float(x) for x in l.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_Row(l):    \n",
    "    #Creation d'un vecteur sparse pour les features\n",
    "    features = Vectors.sparse(784,dict([(i,v) for i,v in enumerate(l[:-1]) if v!=0]))\n",
    "    row = Row(label = l[-1], features= features)\n",
    "    return row\n",
    "def Split_per(j):\n",
    "            def Split_per_index(i, iter):\n",
    "                return iter if i == j else []\n",
    "            return Split_per_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainRDD.map(list_to_Row).toDF()\n",
    "testDF = testRDD.map(list_to_Row).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainDF.select(trainDF.columns[0])\n",
    "labels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainDF.select(trainDF.columns[1])\n",
    "X.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian_mixture_reduction :\n",
    "    \n",
    "    def __init__(self ) :\n",
    "        self.map_means , self.map_covs , self.map_weights , self.Means , self.Covaes ,  self.Weights = [None]*6\n",
    "        \n",
    "    def fit(self , X  , n_components , n_partition):\n",
    "        self.X            = X\n",
    "        self.X.cache()\n",
    "        N = self.X.count()\n",
    "        self.n_components = n_components\n",
    "        self.n_partition  = n_partition\n",
    "        Means_global, Covars_global , Weights_global= [[]]*3\n",
    "        def Split_per(j):\n",
    "            def Split_per_index(i, iter):\n",
    "                return iter if i == j else []\n",
    "            return Split_per_index\n",
    "        global_time_start = time.time()\n",
    "        for n_per in range(self.X.rdd.getNumPartitions()):\n",
    "            start = time.time()\n",
    "            rdd_per = (self.X).rdd.mapPartitionsWithIndex(Split_per(n_per)).coalesce(1).repartition(n_partition).toDF() ; M= rdd_per.rdd.count()\n",
    "            model =  GaussianMixture().setK(10).setSeed(538009335).fit(rdd_per) ; rdd_per = None \n",
    "            Means_global.append( [model.gaussians[k].mu.array for k in range(self.n_components)  ])\n",
    "            Covars_global.append( [model.gaussians[i].sigma.toArray() for i in range(self.n_components) ])\n",
    "            Weights_global.append( np.array(model.weights)* (M/N )) ; model = None \n",
    "            end = time.time() \n",
    "            print ('fitting partitioned model{0} take '.format(i) , end - start , \"seconde\")\n",
    "        global_time_end = time.time()\n",
    "        print ('training take  ', global_time_end- global_time_start  , \"seconde\")\n",
    "        self.map_means = np.array(Means_global)\n",
    "        self.map_covs = np.array( Covars_global)\n",
    "        self.map_weights = np.array( Weights_global)\n",
    "        start = time.time()\n",
    "        self.majorization_minimization() \n",
    "        end = time.time()\n",
    "        print ('GMR algorithm take  ', global_time_end- global_time_start  , \"seconde\")\n",
    "        self.Means_broadcast = sc.broadcast(tuple(map(DenseVector, self.Means))) \n",
    "        self.Covars_broadcast = sc.broadcast(tuple(map(lambda x : DenseMatrix(2,2, x) , list(map(np.ravel ,self.Covars )) )))\n",
    "        self.Weights_broadcast = sc.broadcast(self.Weights.tolist()) \n",
    "        return self\n",
    "        \n",
    "    def Gaussian_distance(self , Gaussian_1 , Gaussian_2 , which=\"W2\"):\n",
    "        mu1, Sigma1 = Gaussian_1 ; mu2, Sigma2 = Gaussian_2\n",
    "        if which == \"KL\":\n",
    "            m0, S0 = Gaussian_1 ;  m1, S1 = Gaussian_2\n",
    "            N = m0.shape[0] ; iS1 = np.linalg.pinv(S1) ; diff = m1 - m0\n",
    "            tr_term   = np.trace(np.dot(iS1 , S0)) ; det_term  = np.log(np.linalg.det(S1)/np.linalg.det(S0))\n",
    "            quad_term = np.dot(np.dot(diff.T , np.linalg.pinv(S1) ), diff) \n",
    "            return .5 * (tr_term + det_term + quad_term - N)\n",
    "        elif which == \"W2\":\n",
    "            if mu1.shape[0] == 1 or mu2.shape[0] == 1: # 1 dimensional\n",
    "                W2_squared = (mu1 - mu2)**2 + (np.sqrt(Sigma1) - np.sqrt(Sigma2))**2\n",
    "                W2_squared = np.asscalar(W2_squared)\n",
    "            else: # multi-dimensional\n",
    "                sqrt_Sigma1 = linalg.sqrtm(Sigma1)\n",
    "                Sigma = Sigma1 + Sigma2 - 2 * linalg.sqrtm(sqrt_Sigma1 @ Sigma2 @ sqrt_Sigma1)\n",
    "                W2_squared = np.linalg.norm(mu1 - mu2)**2 + np.trace(Sigma) + 1e-13\n",
    "                return np.sqrt(W2_squared)\n",
    "        else:\n",
    "            raise ValueError(\"This ground distance is not implemented!\")\n",
    "        \n",
    "    def Joint_Distribution(self):\n",
    "        K   = self.n_components ;   MK  = self.n_partition*self.n_components ; dis = np.zeros((MK,K)) ; PI  = np.zeros_like(dis) \n",
    "        for i in range(MK):\n",
    "            dis[i , :]= np.array(list(map(lambda aggr_params : np.absolute(self.Gaussian_distance([self.map_means[i], \n",
    "                       self.map_covs[i]], aggr_params , which=\"KL\")) , zip(self.Means, self.Covars))))\n",
    "            PI[i,np.argmin(dis[i, :], axis=0)] = self.map_weights[i]\n",
    "        distance = (PI *dis).sum()\n",
    "        return  PI , distance\n",
    "    \n",
    "    def majorization_minimization(self) :\n",
    "        K   = self.n_components ; MK  = self.n_partition*self.n_components ; d = self.d\n",
    "        self.Means  = KMeans(n_clusters = self.n_components, random_state=0).fit(self.map_means).cluster_centers_\n",
    "        self.Covars = np.full((self.n_components,d,d) , np.identity(d)) \n",
    "        for it in range(self.n_iters):\n",
    "            PI , dis = self.Joint_Distribution() \n",
    "            PI_mu= (PI/PI.sum(0))\n",
    "            for k in range(self.n_components):\n",
    "                self.Means[k]  = np.dot(PI_mu[:,k] , self.map_means)\n",
    "                self.Covars[k] = np.sum(PI_mu[:,k][:, np.newaxis , np.newaxis]*self.map_covs, axis =0)\n",
    "                + np.sum(PI_mu[:,k][:, np.newaxis ,np.newaxis ] * np.array(list(map(lambda row : row[:,np.newaxis]*row, \n",
    "                                                self.map_means-self.Means[k]))) ,axis = 0)\n",
    "        self.Weights = PI.sum(0)\n",
    "        return self.Means , self.Covars ,  self.Weights\n",
    "    \n",
    "    def score(self , data ):\n",
    "        Means = self.Means ; Covars = self.Covars ; Weights = self.Weights ; n_components = self.n_components\n",
    "        def local_score( data , Means = Means , Covars = Covars , Weights = Weights , n_components = n_components):\n",
    "            data = np.vstack(data)\n",
    "            if data.ndim == 1: data = data[:, np.newaxis]\n",
    "            if data.size == 0: return np.array([]), np.empty((0, n_components))\n",
    "            if data.shape[1] != Means.shape[1]: raise ValueError('The shape of x is not compatible with self')\n",
    "            min_covar=1.e-7 ; n_samples, n_dim = data.shape ; nmix = n_components ; log_prob = np.empty((n_samples, nmix))\n",
    "            for c, (mu, cv) in enumerate(zip(Means, Covars)):\n",
    "                try: cv_chol = linalg.cholesky(cv, lower=True)\n",
    "                except linalg.LinAlgError:\n",
    "                    try: cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),lower=True)\n",
    "                    except linalg.LinAlgError: raise ValueError(\"'covars' must be symmetric, \"\"positive-definite\")\n",
    "                cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))\n",
    "                cv_sol = linalg.solve_triangular(cv_chol, (data - mu).T, lower=True).T\n",
    "                log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) + n_dim * np.log(2 * np.pi) + cv_log_det)\n",
    "            log_prob +=np.log(Weights) ; log_prob = logsumexp(log_prob, axis=1)\n",
    "            return log_prob\n",
    "        return data.mapPartitions(local_score).collect()\n",
    "        \n",
    "    def predictSoft(self, x):\n",
    "        weights =self.Weights_broadcast.value  ; means = self.Means_broadcast.value ; sigmas =self.Covars_broadcast.value\n",
    "        if isinstance(x, RDD):\n",
    "            membership_matrix = callMLlibFunc(\"predictSoftGMM\", x.map(_convert_to_vector), _convert_to_vector(weights), means, sigmas)\n",
    "            return membership_matrix.map(lambda x: pyarray.array('d', x))\n",
    "        else:\n",
    "            raise TypeError(\"data should be a RDD, received %s\" % type(x)) \n",
    "            \n",
    "    def predict(self, x):\n",
    "        if isinstance(x, RDD):\n",
    "            cluster_labels = self.predictSoft(x).map(lambda z: z.index(max(z)))\n",
    "            return cluster_labels.collect()\n",
    "        else:\n",
    "            raise TypeError(\"data should be a RDD, received %s\" % type(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gaussian_mixture_reduction().fit(X , 10 , 12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
