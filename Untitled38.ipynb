{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled38.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/rMzNp1yyilXTtrKMfNaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdelmounim1996/Distributed-Learning-of-Finite-Gaussian-Mixtures/blob/main/Untitled38.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnFQwQLVTIcd",
        "outputId": "29ecec3f-4aa4-4634-f374-e008b5f55194"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 55.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gI_iQPfTJJp"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG9ahZzuTJRQ"
      },
      "source": [
        "\"\"\"machaine learning  library\"\"\"\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuGnVy1ATJUT"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('dark_background')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Qb7WLtTJXQ"
      },
      "source": [
        "from sklearn.datasets import make_blobs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9F0eZR-Tt-P"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.rdd import RDD\n",
        "from pyspark.mllib.linalg import  DenseVector , DenseMatrix , _convert_to_vector\n",
        "from pyspark.mllib.common import callMLlibFunc\n",
        "import array as pyarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mcOnY_UTJaI"
      },
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\") \\\n",
        "                    .appName('Distributed Learning of Finite Gaussian Mixtures') \\\n",
        "                    .getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70QIJH-NTJdA"
      },
      "source": [
        "n_samples_ = 10000\n",
        "k = 10\n",
        "X, y_true = make_blobs(n_samples=n_samples_,\n",
        "                       n_features = 2 , centers=k,\n",
        "                       cluster_std=0.5,\n",
        "                       random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7xpVJudTJgC"
      },
      "source": [
        "data_make = sc.parallelize(X , 4 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MgZgXg1TJiz"
      },
      "source": [
        "class EM_pMLE:\n",
        "    def __init__(self ,  n_components  ):\n",
        "        self.n_components = n_components\n",
        "\n",
        "    def fit(self , X  : int , n_iters = 500 , tol = 0.01 , init_params = None ):\n",
        "        self.X            = X\n",
        "        self.n_iters      = n_iters\n",
        "        self.tol          = tol\n",
        "        self.init_params  = init_params\n",
        "        self.N , self.d   = self.X.shape\n",
        "        self.converged    = False\n",
        "        self.S_x          = np.cov(self.X , rowvar=False)\n",
        "        self.Means   = None \n",
        "        self.Covars  = None \n",
        "        self.Weights = None\n",
        "        np.seterr(divide='ignore', invalid='ignore')\n",
        "        \n",
        "        # initializations \n",
        "        if  self.init_params is  None:\n",
        "          start = time.time()\n",
        "          self.Means        = KMeans( n_clusters = self.n_components, random_state=0).fit(self.X).cluster_centers_\n",
        "          self.Covars       = np.full((self.n_components , self.d , self.d),np.identity(self.d) ) \n",
        "          self.Weights      = np.full( shape=self.n_components , fill_value=1./self.n_components)\n",
        "          end = time.time()\n",
        "          print(\"kmeans sklearn time : \", end - start)\n",
        "        else :\n",
        "          self.Means   = self.init_params[0]\n",
        "          self.Covars  = self.init_params[1]\n",
        "          self.Weights = self.init_params[2]\n",
        "        \n",
        "        # start algorithm : \n",
        "        dis_likelihood = [[np.infty , np.infty] ]\n",
        "        for it in range(self.n_iters):\n",
        "            self.likelihood ( self.X  )\n",
        "            dis_likelihood.append(self.penalized_likelihood())\n",
        "            if abs(dis_likelihood[it+1][1]-dis_likelihood[it][1]) <= self.tol : \n",
        "                self.converged = True\n",
        "                self.max_iters = it \n",
        "                break\n",
        "            self.E_step() \n",
        "            self.M_step()\n",
        "        self.max_likelihood = dis_likelihood[1:]\n",
        "        # end algorithm\n",
        "        return self\n",
        "\n",
        "    def likelihood (self , data  ):\n",
        "        self.likelihood_weighted = np.asarray(\n",
        "        [  multivariate_normal(mu , sigma , allow_singular = True).pdf( data) for mu , sigma in zip(self.Means , self.Covars) ])* self.Weights[: , np.newaxis]\n",
        "\n",
        "    def penalized_likelihood(self):\n",
        "        an =1./np.sqrt(self.N) \n",
        "        log_likelihood_EM_pLME  = (np.log( self.likelihood_weighted)[(self.likelihood_weighted ==self.likelihood_weighted.max(0))]).sum()\n",
        "        log_likelihood_EM       = np.log(     self.likelihood_weighted.sum(0)       ).sum() \n",
        "        penalty_quantity        = (an*( np.trace(self.S_x*np.linalg.pinv(self.Covars), axis1 = 1 , axis2 = 2) + np.log(np.linalg.det(self.Covars)) ) ).sum()\n",
        "        return log_likelihood_EM_pLME - penalty_quantity , log_likelihood_EM-penalty_quantity\n",
        "\n",
        "    def E_step(self) :\n",
        "        self.conditional_expectation =  self.likelihood_weighted  / self.likelihood_weighted .sum(0) ; self.likelihood_weighted = None \n",
        "\n",
        "    def M_step(self):\n",
        "        an = 1/np.sqrt(self.N) \n",
        "        self.Weights =  (1./self.N)*self.conditional_expectation.sum(1)\n",
        "        self.Means = np.dot(self.conditional_expectation , self.X)*(  1/(self.N*self.Weights))[:,np.newaxis]\n",
        "        \n",
        "        Mu = self.X -self.Means[:, np.newaxis]\n",
        "        S = np.add.reduce((self.conditional_expectation)[:,:, np.newaxis , np.newaxis] * \\\n",
        "                        (np.repeat( Mu, self.d , axis = 1 ).reshape(-1,  self.d )*\\\n",
        "                         Mu.ravel()[: , np.newaxis] ).reshape(self.n_components ,-1 ,self.d  , self.d )\n",
        "                         , axis = 1 )\n",
        "        self.conditional_expectation = None \n",
        "        self.Covars = (1./(2*an + self.N*self.Weights))[:,np.newaxis , np.newaxis]*(2*an*self.S_x + S)\n",
        "  \n",
        "    def predict(self , data ):   \n",
        "        self.likelihood( data )\n",
        "        return self.likelihood_weighted .argmax(0)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfKLxOVRkIyv"
      },
      "source": [
        "%%time\n",
        "model = EM_pMLE(k).fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w57Qk6BR1t9D"
      },
      "source": [
        "%%time\n",
        "\"\"\"predictions data testing  \"\"\"\n",
        "labels = model.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8zvvSSG1yOo"
      },
      "source": [
        "\"\"\"test convergence \"\"\"\n",
        "model.converged , model.max_iters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkEYIB9B1ySn"
      },
      "source": [
        "\"\"\"Plotting data \"\"\"\n",
        "fig = plt.figure(figsize=(16,6))\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax1.scatter(X[:,0], X[:,1], s = 10  ,c = labels , cmap='viridis')\n",
        "ax2.scatter(X[:,0], X[:,1], s = 10 , c = y_true, cmap='viridis')\n",
        "ax1.set_title('EM_pMLE clustering')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax2.set_title('Real clusters')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpits6ma1yhl"
      },
      "source": [
        "\n",
        "def Kullback_Leibler_Distance(G1 , G2 ):\n",
        "  Means_1 , Covars_1 = G1\n",
        "  Means_2 , Covars_2 = G2\n",
        "  Sigma_inv = np.linalg.pinv(Covars_2)\n",
        "  k,d = Means_2.shape ; m, d = Means_1.shape ;  KL_dis_gmm = np.empty((m, k))\n",
        "  xdiff = Means_1-Means_2[0]\n",
        "  for i in range(k):\n",
        "    xdiff = Means_1-Means_2[i]\n",
        "    KL_dis_gmm[:, i] = 0.5*(  np.log(np.abs( np.linalg.det(Covars_2[i])/ np.linalg.det(Covars_1)) )+ np.trace(np.dot( Covars_1 , Sigma_inv[i]  ) , axis1 = 1 , axis2 = 2 )  \n",
        "  +  (xdiff.dot( Sigma_inv[i])*xdiff).sum(axis=-1) -d ) \n",
        "  return KL_dis_gmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo2BO1fcTJly"
      },
      "source": [
        "class  Gaussian_mixture_reduction :\n",
        "    \n",
        "    def __init__(self , n_components : int   ):\n",
        "        self.n_components = n_components\n",
        "        \n",
        "    def fit(self , X :RDD   , n_batch : int  , n_partition : int = None ,  Parallel_Estimator : str = 'EM_pMLE' , n_iters  : int = 10 , tol : float = 0.000001 ):\n",
        "        #<<_____________________start time ____________________>>#\n",
        "        start = time.time()   \n",
        "        self.n_batch                = n_batch\n",
        "        self.n_partition            = n_partition\n",
        "        self.n_iters                = n_iters\n",
        "        self.tol                    = tol \n",
        "        self.Parallel_Estimator     = Parallel_Estimator\n",
        "        # check parameters : \n",
        "        # ================\n",
        "        if isinstance(X, RDD):\n",
        "            X.cache()\n",
        "            self.N = X.count()\n",
        "            ls = X.first()\n",
        "\n",
        "            if isinstance(ls  , list ):\n",
        "              self.d = len(X.first())\n",
        "            else :\n",
        "              self.d = X.first().size\n",
        "\n",
        "            if self.n_partition is None :\n",
        "              self.n_partition = X.getNumPartitions()\n",
        "\n",
        "            elif isinstance(self.n_partition , int)  :\n",
        "              if self.n_partition  < X.getNumPartitions() and  self.n_partition >= 1 :\n",
        "                X.repartition(self.n_partition) \n",
        "              elif X.getNumPartitions() < self.n_partition :\n",
        "                X.coalesce(self.n_partition)\n",
        "            else : \n",
        "              raise ValueError('Invalid value for n_partition : %s' % self.n_partition) \n",
        "        else : \n",
        "          raise TypeError(\"data should be a RDD, received %s\" % type(X)) \n",
        "\n",
        "        if self.n_iters < 1: \n",
        "          raise ValueError('estimation requires at least one run')\n",
        "        if self.tol < 0.:\n",
        "           raise ValueError('Invalid value for covariance_type: %s' %tol)\n",
        "\n",
        "        end = time.time()\n",
        "        print(\"processing data time : \" , end -start )\n",
        "        # <<_________________end time processing____________>> #\n",
        "\n",
        "        # parallelized distributed estimator : \n",
        "        #=====================================\n",
        "        n_clusters = self.n_components \n",
        "        if self.Parallel_Estimator == 'EM_pMLE':\n",
        "\n",
        "          def local_estimator(  partitioned_data  , n_components = n_clusters , init_params= None  ):\n",
        "            import numpy as np\n",
        "            partitioned_data= np.vstack(partitioned_data) \n",
        "            n_samples_partition = partitioned_data.shape[0]\n",
        "            if  init_params is None :\n",
        "              model = EM_pMLE(n_components).fit(partitioned_data )\n",
        "            else :\n",
        "              model = EM_pMLE(n_components).fit(partitioned_data ,  init_params)\n",
        "            return   [ model.Means , model.Covars , model.Weights  *n_samples_partition ]\n",
        "\n",
        "        elif  self.Parallel_Estimator == 'GMM':\n",
        "\n",
        "          def local_estimator(  partitioned_data  , n_components =  n_clusters , init_params= None ):\n",
        "            import numpy as np\n",
        "            from sklearn.mixture import GaussianMixture \n",
        "            partitioned_data= np.vstack(partitioned_data) \n",
        "            n_samples_partition = partitioned_data.shape[0]\n",
        "            if  init_params is None :\n",
        "              model = GaussianMixture(n_components = n_components , random_state=0).fit( partitioned_data  ) \n",
        "            else :\n",
        "              model = GaussianMixture(n_components = n_components , random_state=0).fit( partitioned_data , init_params = init_params  ) \n",
        "             \n",
        "            return   [ model.means_  , model.covariances_ , model.weights_  *n_samples_partition ]\n",
        "\n",
        "        else : \n",
        "          raise('this estimator is not exist !!!')\n",
        "        \n",
        "        \n",
        "        # Get parametres of the weighted average distribution :\n",
        "        # =====================================================\n",
        "        start_AVG = time.time()\n",
        "        list_rdd = X.randomSplit([1/self.n_batch]*self.n_batch) ; X.unpersist()\n",
        "        lst_means = [] ; lst_covs = [] ; lst_weights = [] \n",
        "    \n",
        "        for i  in range(self.n_batch) :\n",
        "          start = time.time()\n",
        "          list_rdd[i].cache()\n",
        "          zip_params =(list_rdd[i]).mapPartitions(local_estimator).collect() ; list_rdd[i].unpersist() ; p=len(zip_params)\n",
        "          lst_means.append([zip_params[i] for i in range( 0, p , 3)])\n",
        "          lst_covs.append([zip_params[i] for i in range( 1, p  , 3)])\n",
        "          lst_weights.append([zip_params[i] for i in range( 2, p , 3)])\n",
        "          end = time.time()\n",
        "          print('---->batch=%.1d, time(s)=%.1f' % ( i , end - start))\n",
        "        self.AVG_means = np.array(lst_means).reshape(-1 , self.d)\n",
        "        self.AVG_covs = np.array(lst_covs).reshape(-1 , self.d , self.d)\n",
        "        self.AVG_weights = np.array(lst_weights).flatten()/self.N\n",
        "        end_AVG = time.time()\n",
        "        print (\"weighted average distribution  time:\", end_AVG - start_AVG)\n",
        "        start = time.time()\n",
        "        self.majorization_minimization() \n",
        "        end = time.time()\n",
        "        print(\"this is time of majorization_minimization (s): \",end-start  )\n",
        "        self.Means_broadcast = sc.broadcast(tuple(map(DenseVector, self.Means))) \n",
        "        self.Covars_broadcast = sc.broadcast(tuple(map(lambda x : DenseMatrix(2,2, x) , list(map(np.ravel ,self.Covars )) )))\n",
        "        self.Weights_broadcast = sc.broadcast(self.Weights.tolist()) \n",
        "        return self\n",
        "    \n",
        "    def majorization_minimization(self) :\n",
        "        K   = self.n_components ; MK  = self.n_partition*self.n_components*self.n_batch ; d = self.d\n",
        "        self.Means  = KMeans(n_clusters = self.n_components, random_state=0).fit(self.AVG_means).cluster_centers_\n",
        "        self.Covars = np.full((self.n_components,d,d) , np.identity(d)) \n",
        "        self.Weights = np.array([1/self.n_components]*self.n_components)\n",
        "        self.trans_divergence = [np.Infinity]\n",
        "        \n",
        "        for it in range(self.n_iters):\n",
        "            print(\"iters : \", it )\n",
        "            start = time.time()\n",
        "            dis = Kullback_Leibler_Distance( (self.AVG_means, self.AVG_covs) , (self.Means , self.Covars)  )\n",
        "            end = time.time()\n",
        "            print(\"KL distance time :\", end-start)\n",
        "            dis_min = dis.min(1) ; args = dis.argmin(1) ; PI = self.AVG_weights[args]\n",
        "            self.trans_divergence.append(   (PI*dis_min).sum()  )\n",
        "\n",
        "            for ind in range(K) : \n",
        "              start = time.time()\n",
        "              PI_ind = PI[args == ind] ; Beta = PI_ind/PI_ind.sum()\n",
        "              self.Means[ind] = ( Beta[: , np.newaxis]*self.AVG_means[args == ind] ).sum(0)\n",
        "              self.Covars[ind] = (Beta[:, np.newaxis , np.newaxis] *( self.AVG_covs[args == ind] \\\n",
        "              + np.einsum('ij , im ->ijm', self.AVG_means[args == ind] - self.Means[ind] ,self.AVG_means[args == ind] - self.Means[ind]  )) ).sum(0)\n",
        "              end = time.time()\n",
        "              print(\"update params time :\", end-start,\"/k = \", ind)\n",
        "            if ( np.absolute( self.trans_divergence[it+1]-self.trans_divergence[it]) <= self.tol or it == self.n_iters-1 ):\n",
        "              for ind in range(K):\n",
        "                self.Weights[ind] = PI[args == ind].sum()\n",
        "              self.max_iters = it \n",
        "              break \n",
        "    \n",
        "    def score(self , data ):\n",
        "        Means = self.Means ; Covars = self.Covars ; Weights = self.Weights ; n_components = self.n_components\n",
        "        def local_score( data , Means = Means , Covars = Covars , Weights = Weights , n_components = n_components):\n",
        "            data = np.vstack(data)\n",
        "            if data.ndim == 1: data = data[:, np.newaxis]\n",
        "            if data.size == 0: return np.array([]), np.empty((0, n_components))\n",
        "            if data.shape[1] != Means.shape[1]: raise ValueError('The shape of x is not compatible with self')\n",
        "            min_covar=1.e-7 ; n_samples, n_dim = data.shape ; nmix = n_components ; log_prob = np.empty((n_samples, nmix))\n",
        "            for c, (mu, cv) in enumerate(zip(Means, Covars)):\n",
        "                try: cv_chol = linalg.cholesky(cv, lower=True)\n",
        "                except linalg.LinAlgError:\n",
        "                    try: cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),lower=True)\n",
        "                    except linalg.LinAlgError: raise ValueError(\"'covars' must be symmetric, \"\"positive-definite\")\n",
        "                cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))\n",
        "                cv_sol = linalg.solve_triangular(cv_chol, (data - mu).T, lower=True).T\n",
        "                log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) + n_dim * np.log(2 * np.pi) + cv_log_det)\n",
        "            log_prob +=np.log(Weights) ; log_prob = logsumexp(log_prob, axis=1)\n",
        "            return log_prob\n",
        "        return data.mapPartitions(local_score).collect()\n",
        "        \n",
        "    def predictSoft(self, x):\n",
        "        weights =self.Weights_broadcast.value  ; means = self.Means_broadcast.value ; sigmas =self.Covars_broadcast.value\n",
        "        if isinstance(x, RDD):\n",
        "            membership_matrix = callMLlibFunc(\"predictSoftGMM\", x.map(_convert_to_vector), _convert_to_vector(weights), means, sigmas)\n",
        "            return membership_matrix.map(lambda x: pyarray.array('d', x))\n",
        "        else:\n",
        "            raise TypeError(\"data should be a RDD, received %s\" % type(x)) \n",
        "            \n",
        "    def predict(self, x):\n",
        "        if isinstance(x, RDD):\n",
        "            cluster_labels = self.predictSoft(x).map(lambda z: z.index(max(z)))\n",
        "            return cluster_labels.collect()\n",
        "        else:\n",
        "            raise TypeError(\"data should be a RDD, received %s\" % type(x)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mX7QsQceJwq"
      },
      "source": [
        "%%time\n",
        "DLFGM = Gaussian_mixture_reduction(k).fit(data_make , 1 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C4KX8oReJze"
      },
      "source": [
        "%%time\n",
        "labels = DLFGM.predict(data_make)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OID2snkUp9Dk"
      },
      "source": [
        "\"\"\"Plotting data \"\"\"\n",
        "fig = plt.figure(figsize=(16,6))\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax1.scatter(X[:,0], X[:,1], s = 10  ,c = labels , cmap='viridis')\n",
        "ax2.scatter(X[:,0], X[:,1], s = 10 , c = y_true, cmap='viridis')\n",
        "ax1.set_title('EM_pMLE clustering')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax2.set_title('Real clusters')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbnKuqUtp9QU"
      },
      "source": [
        "\"\"\"Means \"\"\"\n",
        "fig = plt.figure(figsize=(18,6))\n",
        "plt.scatter(DLFGM.AVG_means[:,0],DLFGM.AVG_means[:,1], s=10, c=\"red\")\n",
        "plt.scatter(DLFGM.Means[:,0], DLFGM.Means[:,1] , s =200 , c =\"blue\" , marker='+')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPtjtoOmp9TV"
      },
      "source": [
        "\"\"\" Loading file library \"\"\"\n",
        "import urllib.request\n",
        "DATA_PATH=\"\"\n",
        "f = urllib.request.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_train.csv\",DATA_PATH+\"mnist_train.csv\")\n",
        "f = urllib.request.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_test.csv\",DATA_PATH+\"mnist_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKqwJMwkp9V8"
      },
      "source": [
        "trainRDD = sc.textFile(DATA_PATH+\"mnist_train.csv\"  ).map(lambda l: [float(x) for x in l.split(',')])\n",
        "\n",
        "trainRDD  = trainRDD.map(lambda x: np.array(x[:-1])+ np.random.normal(0.,0.05, 784 ) ,4 ) \n",
        "print(\"shape trainRDD : ( \",trainRDD.count() ,\",\", len(trainRDD.first()) , \")\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csab9fWmp9Yo"
      },
      "source": [
        "%%time\n",
        "model = Gaussian_mixture_reduction(10).fit(trainRDD , 6 , Parallel_Estimator  = 'GMM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwMndQRYp9bj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW0lKueXeJ5y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}